---
title: "Factorial designs: power from means and SD"
author: Marcello Gallucci 
topic: factorial
category: rosetta
nickname: ros_fac_means
linklabel: "Rosetta: Factorial designs from means" 
bibliography: 
     - bib.bib
link-citations: true
        
editor_options: 
  chunk_output_type: console
  
---


```{r echo=FALSE,results='hide'}
library(mcdocs)
mcdocs_init()

```

`r version("0.2.0")`

Here we compare the results of `r modulename()` with other software that performs power analysis. In particular, we will compare our results with [R pwr package](https://cran.r-project.org/web/packages/pwr/pwr.pdf),  `r ext_url("Superpower R package","https://aaroncaldwell.us/SuperpowerBook/")`, and  [G*Power](https://www.psychologie.hhu.de/arbeitsgruppen/allgemeine-psychologie-und-arbeitspsychologie/gpower). 

# Data

`Factorial designs power analysis` from `Expected Means` of `r modulename()` requires a dataset with the expected means and the standard deviations for the desin cells one is planning. Factors of the design should be listed as categorical factors, and their means and sd should be listed in subsequent columns. For instance, a simple design with two groups, can be prepared like this:

`r pic("rosetta/pics/factorial/facmeans.1.png")`

When data are defined, one can use the `Factorial designs power analysis` from `Expected Means` interface in define the design factors, their means and standard deviations. 

`r pic("rosetta/pics/factorial/facmeans.2.png")`

As soon as the variables are defined, the effect sizes are computed together with the power parameters. 

`r pic("rosetta/pics/factorial/facmeans.3.png")`


As a quick test, we can use `G*Power` to check the results, using its interface for inputing the means and the standard deviations.

`r pic("rosetta/pics/factorial/facmeans.4.png")`

Results are  the same.

Unfortunately, G*Power does not allow to input means and sd other than for one-way between-subjects designs, so we cannot test `r modulename()` for more complex designs. For this purpose, we can use `r ext_url("Superpower R package","https://aaroncaldwell.us/SuperpowerBook/")`, which allows computing the expected power starting from an array of means and standard deviations.

# Between-subjects ANOVA 

We can use the dataset `factorial` which comes with `r modulename()` in the `r jamovi` data library.

`r pic("rosetta/pics/factorial/facmeans.5.png")`

Data describe the means and standard deviations for a 2X3X2 design. 

`r pic("rosetta/pics/factorial/facmeans.data.png")`

In `Factorial designs power analysis` from `Expected Means` we can simply input the factors, indicating also the column representing the means and the one representing the standard devations. 

`r pic("rosetta/pics/factorial/facmeans.6.png")`

For the purpose of computing the expected power, we can set the sample size to 240, corresponding to 20 cases per cell.

`r pic("rosetta/pics/factorial/facmeans.7.png")`

`r modulename()` computes the expected effect sizes and the expected power for all factors effects and their interaction.


For comparison, we can use `Superpower` employing the following code:


```{r}


ds<-pamlj::factorial
std<-8
n<-20
des_str<-"2b*3b*2b"
design <- Superpower::ANOVA_design(design = des_str, 
                              #sample size per group 
                              n = n, 
                              #pattern of means
                              mu =ds$means,
                              sd = std,
                              plot=FALSE
                       ) 
exact_result <- Superpower::ANOVA_exact(design,
                            alpha_level = .05,
                            verbose = FALSE)
zapsmall(exact_result$main_results)


```


It is easy to check that the expected power is almost identical (apart for rounding) for all effects. It is worth noticing that despite the expected power
is identical for the two software, they produce different effect size indices. This is due to the way the two software compute the effect sizes. `Superpower` builds a dataset of the input sample size and estimate the effect sizes, whereas `r modulename()` computes the estimated effect sizes in the population. Because these effect size indices are upward biased, in small to moderate sample sizes they tend to be larger than in the population. Indeed, if we ask `Superpower` to compute the effect sizes for a very large sample, the estimated indices converge to the ones obtained in `r modulename()`.

```{r}

ds<-pamlj::factorial
std<-8
n<-5000
des_str<-"2b*3b*2b"
design <- Superpower::ANOVA_design(design = des_str, 
                              #sample size per group 
                              n = n, 
                              #pattern of means
                              mu =ds$means,
                              sd = std,
                              plot=FALSE
                       ) 
exact_result <- Superpower::ANOVA_exact(design,
                            alpha_level = .05,
                            verbose = FALSE)
zapsmall(exact_result$main_results)


```


# Repeated measures designs

Let's now assume that the design we have analyzed before was a repeated measures design. In repeated measures designs, to compute the power parameters from means and standard deviations, one needs to anticipate the average correlations among repeated measures. Let's assume that the repeated measures correlate, on average, .2. We can setup the analysis as before, but in the `Repeated Measures` tab, we can declare which factor is within-subjects and input the expected correlation. In this example, we set the sample size to 50.

`r pic("rosetta/pics/factorial/facmeans.8.png")`

Having set the factors as repeated measures factors and with a non-zero correlation changes the effect size estimation and the estimated power.

`r pic("rosetta/pics/factorial/facmeans.9.png")`

We should also note that the residuals degrees of freedom are now different, because they depend on the sample size and degrees of freedom of the effect. 

For comparison, we run the same analysis with `Superpower`.


```{r}

ds<-pamlj::factorial
std<-8
n<-50
des_str<-"2w*3w*2w"
r<-.2
design <- Superpower::ANOVA_design(design = des_str, 
                              #sample size per group 
                              n = n, 
                              #pattern of means
                              mu =ds$means,
                              r=r,
                              sd = std,
                              plot=FALSE
                       ) 
exact_result <- Superpower::ANOVA_exact(design,
                            alpha_level = .05,
                            verbose = FALSE)
zapsmall(exact_result$main_results)


```

We can see that the results are pretty similar. The difference is due to the fact that `Superpower` estimates power based on the sample estimates of the effect size, whereas `r modulename()` uses the population effect size. To add an additional proof, we can ask GPower to compute the power for a Repeated measure effect with effect size $p\eta^2=.0818$, 1 DF (two measures), all within effects (1 group) with $N=50$ (and SPSS effect size, cf. `r link_pages("ros_fac_peta")`).

`r pic("rosetta/pics/factorial/facmeans.10.png")`

Results are indeed identical to `r modulename()` results.

# Mixed ANOVA

## Expected Power

We can now test the module with a mixed design, with one repeated measure factor (say A) and the other two (B and C) as between-subjects factors. This yields a design with 6 groups, so we can set the sample size to 120, corresponding to 20 cases per group. As for the correlations among repeated measures, we can set it to .5. In PAMLj, we simply set up the input as follows.

`r pic("rosetta/pics/factorial/facmeans.11.png")`

and for the repeated measures we have 

`r pic("rosetta/pics/factorial/facmeans.12.png")`

Results are the following:

`r pic("rosetta/pics/factorial/facmeans.13.png")`

For comparison, we run the same analysis with `Superpower`.


```{r}

ds<-pamlj::factorial
std<-8
n<-50
des_str<-"2w*3b*2b"
r<-.5
design <- Superpower::ANOVA_design(design = des_str, 
                              #sample size per group 
                              n = n, 
                              #pattern of means
                              mu =ds$means,
                              r=r,
                              sd = std,
                              plot=FALSE
                       ) 
exact_result <- Superpower::ANOVA_exact(design,
                            alpha_level = .05,
                            verbose = FALSE)
zapsmall(exact_result$main_results)


```

All considering, results are enough similar. 

## Required N

To validate PAMLj computation of the required sample size, we can use GPOWER. Using the `factorial` dataset, assuming one repeated measure factor (A) with correlation equal to .5,  the other two (B and C) as between-subjects factors, with required power set to .90, we obtain the following results.

`r pic("rosetta/pics/factorial/facmeans.14.png")`

We can now pick the interaction $A*B$, with $p\eta^2=.0293$ with 2 degrees of freedom. In GPower, we can use  `ANOVA: Repeated measures, within-between interactions`, with `Options` set to `SPSS`. To approximate the results, we can set the `Number of repeated measures` to 2,
and the `Number of groups` to 3 (factor B has three levels). 

`r pic("rosetta/pics/factorial/facmeans.15.png")`

The results are similar but not very close, being the $N=219$ for PAMLj and $N=216$ for GPower. However, in this case it is not a matter of approximation or rounding. Notice that both software indicates the same effect size and the same degrees of freedom, PAMLj gives a slightly larger required N because PAMLj takes into the account that the whole model  has three factors, whereas this information cannot be passed to GPower. In fact, PAMLj yields the same results of GPower if one selects the option `Liberal` in the `Non-centrality parameter`. The option `Liberal`, in fact, estimates the required N without considering the whole model degrees of freedom, but only the effect degrees of freedom (cf `r link_pages(nickname="specs_fncp")`). 

`r pic("rosetta/pics/factorial/facmeans.16.png")`
`r pic("rosetta/pics/factorial/facmeans.17.png")`



`r issues()`

`r backto("rosetta")`

