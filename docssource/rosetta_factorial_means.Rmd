---
title: "Factorial designs: power from means and SD"
topic: samplesize
category: rosetta
nickname: ros_fac_means
linklabel: "Rosetta: GLM required N" 
output: 
  html_document:
     includes:
         in_header: ganalytics.txt
     toc: true
     toc_float:
        collapsed: false
bibliography: 
     - bib.bib
link-citations: true
        
editor_options: 
  chunk_output_type: console
---


```{r echo=FALSE,results='hide'}
library(mcdocs)
mcdocs_init()

```

# Power Analysis for the Factorial Designs

`r version("0.2.0")` 

Here we compare the results of `r modulename()` with other software that performs power analysis. In particular, we will compare our results with [R pwr package](https://cran.r-project.org/web/packages/pwr/pwr.pdf),  `r ext_url("Superpower R package","https://aaroncaldwell.us/SuperpowerBook/")`, and  [G*Power](https://www.psychologie.hhu.de/arbeitsgruppen/allgemeine-psychologie-und-arbeitspsychologie/gpower). 

# Data

`Factorial designs power analysis` from `Expected Means` of `r modulename()` requires a dataset with the expected means and the standard deviations for the desin cells one is planning. Factors of the design should be listed as categorical factors, and their means and sd should be listed in subsequent columns. For instance, a simple design with two groups, can be prepared like this:

`r pic("rosetta/pics/factorial/facmeans.1.png")`

When data are defined, one can use the `Factorial designs power analysis` from `Expected Means` interface in define the design factors, their means and standard deviations. 

`r pic("rosetta/pics/factorial/facmeans.2.png")`

As soon as the variables are defined, the effect sizes are computed together with the power parameters. 

`r pic("rosetta/pics/factorial/facmeans.3.png")`


As a quick test, we can use `G*Power` to check the results, using its interface for inputing the means and the standard deviations.

`r pic("rosetta/pics/factorial/facmeans.4.png")`

Results are almost the same, with only one unit of difference.

Unfortunately, G*Power does not allow to input means and sd other than for one-way between-subjects designs, so we cannot test `r modulename()` for more complex designs. For this purpose, we can use `r ext_url("Superpower R package","https://aaroncaldwell.us/SuperpowerBook/")`, which allows computing the expected power starting from an array of means and standard deviations.

# Between-subjects ANOVA 

We can use the dataset `factorial` which comes with `r modulename()` in the `r jamovi` data library.

`r pic("rosetta/pics/factorial/facmeans.5.png")`

Data describe the means and standard deviations for a 2X3X2 design. 

`r pic("rosetta/pics/factorial/facmeans.data.png")`

In `Factorial designs power analysis` from `Expected Means` we can simply input the factors, indicating also the column representing the means and the one representing the standard devations. 

`r pic("rosetta/pics/factorial/facmeans.6.png")`

For the purpose of computing the expected power, we can set the sample size to 240, corresponding to 20 cases per cell.

`r pic("rosetta/pics/factorial/facmeans.7.png")`

`r modulename()` computes the expected effect sizes and the expected power for all factors effects and their interaction.


For comparison, we can use `Superpower` employing the following code:


```{r}


ds<-pamlj::factorial
std<-8
n<-20
des_str<-"2b*3b*2b"
design <- Superpower::ANOVA_design(design = des_str, 
                              #sample size per group 
                              n = n, 
                              #pattern of means
                              mu =ds$means,
                              sd = std,
                              plot=FALSE
                       ) 
exact_result <- Superpower::ANOVA_exact(design,
                            alpha_level = .05,
                            verbose = FALSE)
zapsmall(exact_result$main_results)


```


It is easy to check that the expected power is almost identical (apart for rounding) for all effects. It is worth noticing that despite the expected power
is identical for the two software, they produce different effect size indices. This is due to the way the two software compute the effect sizes. `Superpower` build a dataset of the input sample size and estimate the effect size, whereas `r modulename()` computes the estimated effect size in the population. Because these effect size are upward biased, in small to moderate sample size they tend to be larger than in the population. Indeed, if we ask `Superpower` to compute the effect sizes for a very large sample, the estimated indices converge to the ones obtained in `r modulename()`.

```{r}

ds<-pamlj::factorial
std<-8
n<-5000
des_str<-"2b*3b*2b"
design <- Superpower::ANOVA_design(design = des_str, 
                              #sample size per group 
                              n = n, 
                              #pattern of means
                              mu =ds$means,
                              sd = std,
                              plot=FALSE
                       ) 
exact_result <- Superpower::ANOVA_exact(design,
                            alpha_level = .05,
                            verbose = FALSE)
zapsmall(exact_result$main_results)


```


# Repeated measures designs

Let's now assume that the design we have analyzed before was a repeated measures design. In repeated measures designs, to compute the power parameters from means and standard deviations, one needs to anticipate the average correlations among repeated measures. Let's assume that the repeated measures correlate, on average, .2. We can setup the analysis as before, but in the `Repeated Measures` tab, we can declare which factor is within-subjects and input the expected correlation. In this example, we set the sample size to 50.

`r pic("rosetta/pics/factorial/facmeans.8.png")`

Having set the factors as repeated measures factors and with a non-zero correlation changes the effect size estimation and the estimated power.

`r pic("rosetta/pics/factorial/facmeans.9.png")`

We should also note that the residuals degrees of freedom are now different, and they depended on the sample size and degrees of freedom of the effect. 

For comparison, we run the same analysis with `Superpower`.


```{r}

ds<-pamlj::factorial
std<-8
n<-50
des_str<-"2w*3w*2w"
r<-.2
design <- Superpower::ANOVA_design(design = des_str, 
                              #sample size per group 
                              n = n, 
                              #pattern of means
                              mu =ds$means,
                              r=r,
                              sd = std,
                              plot=FALSE
                       ) 
exact_result <- Superpower::ANOVA_exact(design,
                            alpha_level = .05,
                            verbose = FALSE)
zapsmall(exact_result$main_results)


```

We can see that the results are pretty similar. The difference is due to the fact that `Superpower` estimates power based on the sample estimates of the effect size, whereas `r modulename()` uses the population effect size. To add an additional proof, we can ask GPower to compute the power for a Repeated measure effect with effect size $p\eta^2=.0818$, 1 DF (two measures), all within effects (1 group) with $N=50$ (and SPSS effect size, cf. `r link_pages("ros_fac_peta")`).

`r pic("rosetta/pics/factorial/facmeans.10.png")`

Results are indeed identical to `r modulename()` results.

`r issues()`

# Back to `r link_pages(nickname="rosetta")`


# References
'